from django.core.management.base import BaseCommand
from core.chatbot.TechChatbot import TechChatbot
from core.chatbot.EmbeddingManager import EmbeddingManager
import time
import json
import os
from datetime import datetime
import torch
import logging

logging.basicConfig(level=logging.INFO)


class Command(BaseCommand):
    help = 'Realiza benchmark completo del sistema de chatbot'

    def add_arguments(self, parser):
        parser.add_argument(
            '--output-file',
            type=str,
            help='Archivo donde guardar los resultados del benchmark'
        )
        parser.add_argument(
            '--detailed',
            action='store_true',
            help='Incluye mÃ©tricas detalladas en el benchmark'
        )
        parser.add_argument(
            '--test-embeddings',
            action='store_true',
            help='Incluye pruebas de rendimiento de embeddings'
        )

    def handle(self, *args, **options):
        start_time = datetime.now()

        self.stdout.write(
            self.style.SUCCESS('âš¡ INICIANDO BENCHMARK DEL CHATBOT')
        )

        results = {
            'benchmark_date': start_time.isoformat(),
            'system_info': self._get_system_info(),
            'tests': {}
        }

        try:
            # 1. Benchmark de Embeddings
            if options['test_embeddings']:
                self.stdout.write('\nğŸ“Š Benchmarking Embeddings...')
                results['tests']['embeddings'] = self._benchmark_embeddings()

            # 2. Benchmark de Chatbot
            self.stdout.write('\nğŸ¤– Benchmarking Chatbot...')
            results['tests']['chatbot'] = self._benchmark_chatbot()

            # 3. Benchmark de BÃºsqueda
            self.stdout.write('\nğŸ” Benchmarking Sistema de BÃºsqueda...')
            results['tests']['search'] = self._benchmark_search()

            # 4. Pruebas de EstrÃ©s
            self.stdout.write('\nğŸ’ª Pruebas de EstrÃ©s...')
            results['tests']['stress'] = self._stress_tests()

            # Calcular mÃ©tricas generales
            end_time = datetime.now()
            results['total_duration'] = (end_time - start_time).total_seconds()
            results['summary'] = self._generate_summary(results)

            # Mostrar resultados
            self._display_results(results)

            # Guardar resultados
            if options['output_file']:
                self._save_results(results, options['output_file'])

        except Exception as e:
            self.stdout.write(
                self.style.ERROR(f'âŒ Error durante benchmark: {str(e)}')
            )
            raise

    def _get_system_info(self):
        """Obtiene informaciÃ³n del sistema"""
        info = {
            'python_version': f"{os.sys.version_info.major}.{os.sys.version_info.minor}",
            'torch_version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
        }

        if torch.cuda.is_available():
            info['gpu_name'] = torch.cuda.get_device_name()
            info['gpu_memory_gb'] = torch.cuda.get_device_properties(0).total_memory / 1e9

        return info

    def _benchmark_embeddings(self):
        """Benchmark del sistema de embeddings"""
        try:
            embedding_manager = EmbeddingManager()

            # Pruebas de bÃºsqueda
            search_queries = [
                "celular samsung",
                "computador gaming",
                "audÃ­fonos inalÃ¡mbricos",
                "televisor 55 pulgadas",
                "laptop para trabajo"
            ]

            search_times = []
            search_results_count = []

            for query in search_queries:
                start = time.time()
                results = embedding_manager.search_products(query, top_k=10)
                end = time.time()

                search_times.append(end - start)
                search_results_count.append(len(results))

            # EstadÃ­sticas
            stats = embedding_manager.get_stats()

            return {
                'total_products_indexed': stats.get('total_products', 0),
                'total_categories': len(stats.get('categories', {})),
                'avg_search_time_ms': sum(search_times) / len(search_times) * 1000,
                'min_search_time_ms': min(search_times) * 1000,
                'max_search_time_ms': max(search_times) * 1000,
                'avg_results_per_search': sum(search_results_count) / len(search_results_count),
                'search_success_rate': sum(1 for count in search_results_count if count > 0) / len(
                    search_results_count) * 100
            }

        except Exception as e:
            return {'error': str(e)}

    def _benchmark_chatbot(self):
        """Benchmark del chatbot completo"""
        try:
            chatbot = TechChatbot()

            # Intentar cargar modelo entrenado, sino usar base
            try:
                chatbot.load_model()
                model_type = "fine_tuned"
            except:
                chatbot.load_model(load_base_only=True)
                model_type = "base_model"

            # Consultas de prueba
            test_queries = [
                "Hola, Â¿quÃ© productos tienes?",
                "Busco celulares Samsung",
                "Â¿Hay computadores en oferta?",
                "Necesito audÃ­fonos baratos",
                "Â¿CuÃ¡l TV me recomiendas?",
                "Productos entre 200 y 500 mil pesos"
            ]

            response_times = []
            response_lengths = []
            successful_searches = 0

            for query in test_queries:
                start = time.time()
                response = chatbot.chat(query)
                end = time.time()

                response_times.append(end - start)
                response_lengths.append(len(response))

                # Verificar si la bÃºsqueda fue exitosa (simplificado)
                if len(response) > 50 and any(
                        word in response.lower() for word in ['producto', 'precio', '$', 'disponible']):
                    successful_searches += 1

            # EstadÃ­sticas de conversaciÃ³n
            conv_stats = chatbot.get_conversation_stats()

            return {
                'model_type': model_type,
                'total_queries_tested': len(test_queries),
                'avg_response_time_s': sum(response_times) / len(response_times),
                'min_response_time_s': min(response_times),
                'max_response_time_s': max(response_times),
                'avg_response_length': sum(response_lengths) / len(response_lengths),
                'successful_searches': successful_searches,
                'success_rate_percent': successful_searches / len(test_queries) * 100,
                'conversation_stats': conv_stats
            }

        except Exception as e:
            return {'error': str(e)}

    def _benchmark_search(self):
        """Benchmark del sistema de bÃºsqueda combinado"""
        try:
            chatbot = TechChatbot()
            try:
                chatbot.load_model()
            except:
                chatbot.load_model(load_base_only=True)

            # Casos de prueba de bÃºsqueda
            search_cases = [
                {"query": "Samsung Galaxy", "expected_type": "celular"},
                {"query": "laptop gamer", "expected_type": "computador"},
                {"query": "productos en descuento", "expected_type": "ofertas"},
                {"query": "menos de 300 mil", "expected_type": "precio"},
                {"query": "audÃ­fonos Sony", "expected_type": "marca_especifica"}
            ]

            intent_accuracies = []
            search_effectiveness = []

            for case in search_cases:
                # Extraer intenciÃ³n
                intent = chatbot._extract_search_intent(case["query"])

                # Verificar precisiÃ³n de la intenciÃ³n (simplificado)
                intent_correct = False
                if case["expected_type"] == "celular" and any(
                        cat in str(intent.get('category', '')) for cat in ['celular']):
                    intent_correct = True
                elif case["expected_type"] == "computador" and any(
                        cat in str(intent.get('category', '')) for cat in ['computador']):
                    intent_correct = True
                elif case["expected_type"] == "ofertas" and intent.get('looking_for_deals'):
                    intent_correct = True
                elif case["expected_type"] == "precio" and (
                        intent['price_range']['min'] or intent['price_range']['max']):
                    intent_correct = True
                elif case["expected_type"] == "marca_especifica" and intent.get('brand'):
                    intent_correct = True

                intent_accuracies.append(intent_correct)

                # Buscar productos
                products = chatbot._search_products(intent, top_k=5)
                search_effectiveness.append(len(products) > 0)

            return {
                'intent_extraction_accuracy': sum(intent_accuracies) / len(intent_accuracies) * 100,
                'search_effectiveness': sum(search_effectiveness) / len(search_effectiveness) * 100,
                'total_test_cases': len(search_cases)
            }

        except Exception as e:
            return {'error': str(e)}

    def _stress_tests(self):
        """Pruebas de estrÃ©s del sistema"""
        try:
            chatbot = TechChatbot()
            try:
                chatbot.load_model()
            except:
                chatbot.load_model(load_base_only=True)

            # Prueba de mÃºltiples consultas consecutivas
            stress_queries = ["busco celulares"] * 20  # 20 consultas idÃ©nticas

            start_time = time.time()
            response_times = []

            for query in stress_queries:
                query_start = time.time()
                chatbot.chat(query)
                query_end = time.time()
                response_times.append(query_end - query_start)

            end_time = time.time()

            # Prueba de memoria
            import psutil
            process = psutil.Process(os.getpid())
            memory_usage_mb = process.memory_info().rss / 1024 / 1024

            return {
                'consecutive_queries': len(stress_queries),
                'total_time_s': end_time - start_time,
                'avg_time_per_query_s': sum(response_times) / len(response_times),
                'memory_usage_mb': memory_usage_mb,
                'performance_degradation': (max(response_times) - min(response_times)) / min(response_times) * 100
            }

        except Exception as e:
            return {'error': str(e)}

    def _generate_summary(self, results):
        """Genera resumen de los resultados"""
        summary = {
            'overall_status': 'good',
            'critical_issues': [],
            'recommendations': []
        }

        # Analizar resultados de embeddings
        if 'embeddings' in results['tests']:
            emb = results['tests']['embeddings']
            if emb.get('avg_search_time_ms', 1000) > 500:
                summary['recommendations'].append('Considere optimizar el Ã­ndice de embeddings')
            if emb.get('search_success_rate', 0) < 80:
                summary['critical_issues'].append('Baja tasa de Ã©xito en bÃºsquedas de embeddings')

        # Analizar resultados del chatbot
        if 'chatbot' in results['tests']:
            chat = results['tests']['chatbot']
            if chat.get('avg_response_time_s', 10) > 5:
                summary['recommendations'].append('Tiempo de respuesta alto, considere optimizaciÃ³n')
            if chat.get('success_rate_percent', 0) < 70:
                summary['critical_issues'].append('Baja tasa de Ã©xito del chatbot')

        # Determinar estado general
        if len(summary['critical_issues']) > 2:
            summary['overall_status'] = 'critical'
        elif len(summary['critical_issues']) > 0:
            summary['overall_status'] = 'warning'

        return summary

    def _display_results(self, results):
        """Muestra los resultados del benchmark"""
        self.stdout.write(
            self.style.SUCCESS('\nğŸ“‹ RESULTADOS DEL BENCHMARK')
        )
        self.stdout.write('=' * 60)

        # InformaciÃ³n del sistema
        sys_info = results['system_info']
        self.stdout.write(f'ğŸ–¥ï¸  Sistema: Python {sys_info["python_version"]}, PyTorch {sys_info["torch_version"]}')
        if sys_info['cuda_available']:
            self.stdout.write(f'ğŸ”¥ GPU: {sys_info["gpu_name"]} ({sys_info["gpu_memory_gb"]:.1f}GB)')
        else:
            self.stdout.write('ğŸ’» CPU: Sin GPU detectada')

        # Resultados de pruebas
        for test_name, test_results in results['tests'].items():
            if 'error' in test_results:
                self.stdout.write(f'\nâŒ {test_name.upper()}: Error - {test_results["error"]}')
                continue

            self.stdout.write(f'\nâœ… {test_name.upper()}:')

            if test_name == 'embeddings':
                self.stdout.write(f'  ğŸ“¦ Productos indexados: {test_results["total_products_indexed"]:,}')
                self.stdout.write(f'  ğŸ·ï¸  CategorÃ­as: {test_results["total_categories"]}')
                self.stdout.write(f'  âš¡ BÃºsqueda promedio: {test_results["avg_search_time_ms"]:.1f}ms')
                self.stdout.write(f'  ğŸ¯ Tasa de Ã©xito: {test_results["search_success_rate"]:.1f}%')

            elif test_name == 'chatbot':
                self.stdout.write(f'  ğŸ¤– Modelo: {test_results["model_type"]}')
                self.stdout.write(f'  âš¡ Respuesta promedio: {test_results["avg_response_time_s"]:.2f}s')
                self.stdout.write(f'  ğŸ“ Longitud promedio: {test_results["avg_response_length"]} caracteres')
                self.stdout.write(f'  ğŸ¯ Tasa de Ã©xito: {test_results["success_rate_percent"]:.1f}%')

            elif test_name == 'search':
                self.stdout.write(f'  ğŸ§  PrecisiÃ³n de intenciÃ³n: {test_results["intent_extraction_accuracy"]:.1f}%')
                self.stdout.write(f'  ğŸ” Efectividad de bÃºsqueda: {test_results["search_effectiveness"]:.1f}%')

            elif test_name == 'stress':
                self.stdout.write(f'  ğŸ’ª Consultas consecutivas: {test_results["consecutive_queries"]}')
                self.stdout.write(f'  â±ï¸  Tiempo total: {test_results["total_time_s"]:.2f}s')
                self.stdout.write(f'  ğŸ§  Memoria usada: {test_results["memory_usage_mb"]:.1f}MB')
                self.stdout.write(f'  ğŸ“‰ DegradaciÃ³n: {test_results["performance_degradation"]:.1f}%')

        # Resumen
        summary = results['summary']
        status_icon = {'good': 'âœ…', 'warning': 'âš ï¸', 'critical': 'âŒ'}[summary['overall_status']]
        self.stdout.write(f'\n{status_icon} ESTADO GENERAL: {summary["overall_status"].upper()}')

        if summary['critical_issues']:
            self.stdout.write('\nğŸš¨ PROBLEMAS CRÃTICOS:')
            for issue in summary['critical_issues']:
                self.stdout.write(f'  â€¢ {issue}')

        if summary['recommendations']:
            self.stdout.write('\nğŸ’¡ RECOMENDACIONES:')
            for rec in summary['recommendations']:
                self.stdout.write(f'  â€¢ {rec}')

        self.stdout.write(f'\nâ±ï¸  DuraciÃ³n total del benchmark: {results["total_duration"]:.2f}s')

    def _save_results(self, results, filename):
        """Guarda los resultados en un archivo JSON"""
        try:
            # Crear directorio si no existe
            output_dir = "data/benchmarks"
            os.makedirs(output_dir, exist_ok=True)

            # Si no se especifica extensiÃ³n, agregar .json
            if not filename.endswith('.json'):
                filename += '.json'

            filepath = os.path.join(output_dir, filename)

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)

            self.stdout.write(f'\nğŸ’¾ Resultados guardados en: {filepath}')

        except Exception as e:
            self.stdout.write(f'âš ï¸ Error guardando resultados: {str(e)}')
